{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and extract the dataset\n",
    "The dataset is downloaded from the provided URL and extracted into the 'Dataset' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip download process of the Dataset\n"
     ]
    }
   ],
   "source": [
    "# Change to False if you want to start the download\n",
    "skip_download = True\n",
    "\n",
    "if not skip_download:\n",
    "    import urllib.request\n",
    "    import tarfile\n",
    "\n",
    "    url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
    "\n",
    "    dataset_folder = './Dataset'\n",
    "    if not os.path.exists(dataset_folder):\n",
    "        os.mkdir(dataset_folder)\n",
    "\n",
    "    dataset_path = os.path.join(dataset_folder, 'dependency_treebank')\n",
    "\n",
    "    if not os.path.exists(dataset_path):\n",
    "        urllib.request.urlretrieve(url, dataset_folder)\n",
    "        print(\"Successful download\")\n",
    "\n",
    "    tar = tarfile.open(dataset_path)\n",
    "    tar.extractall(dataset_folder)\n",
    "    tar.close()\n",
    "    print(\"Successful extraction\")\n",
    "else:\n",
    "    print(\"Skip download process of the Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data\n",
    "The data (199 samples in total) is split into a train, validation and test set:\n",
    "- 100 train samples\n",
    "- 50 validation samples\n",
    "- 49 test samples \n",
    "The sets are stored in data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199it [00:01, 159.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pierre</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vinken</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>CD</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>years</td>\n",
       "      <td>NNS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    token  pos  split\n",
       "0  Pierre  NNP  train\n",
       "1  Vinken  NNP  train\n",
       "2       ,    ,  train\n",
       "3      61   CD  train\n",
       "4   years  NNS  train"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define split size\n",
    "TRAIN_SPLIT = 100\n",
    "VAL_SPLIT = 150\n",
    "\n",
    "# Define file iterator\n",
    "def file_iterator():\n",
    "    data_dir = Path('dependency_treebank')\n",
    "    for data_file in filter(lambda f: os.path.isfile(data_dir/f) and f.endswith('.dp'), os.listdir(data_dir)):\n",
    "        yield data_dir/data_file\n",
    "\n",
    "# Create train, val and test set\n",
    "data_set = []\n",
    "split_indexes = []\n",
    "\n",
    "# Iterate over files and perform split\n",
    "for file in tqdm(file_iterator()):\n",
    "    data_set.append(pd.read_csv(file, sep=\"\\t\", names=['token', 'pos'], usecols=[0, 1], engine='python'))\n",
    "    split_indexes.append(len(data_set[-1]))\n",
    "\n",
    "data_frame = pd.concat(data_set)\n",
    "\n",
    "split = ['train']*sum(split_indexes[0:TRAIN_SPLIT]) \\\n",
    "        + ['val']*sum(split_indexes[TRAIN_SPLIT:VAL_SPLIT]) \\\n",
    "        + ['test']*sum(split_indexes[VAL_SPLIT:len(data_set)])\n",
    "\n",
    "assert len(split) == data_frame.shape[0]\n",
    "\n",
    "data_frame['split'] = split\n",
    "\n",
    "train_frame = data_frame[data_frame['split']=='train']\n",
    "test_frame =  data_frame[data_frame['split']=='test']\n",
    "val_frame =   data_frame[data_frame['split']=='val']\n",
    "\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the structure of the constructed data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and padding\n",
    "The data frames are now tokenized and additional padded to have a unit input length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine to tokenize and pad data to unit length\n",
    "def tokenize_and_pad(data_frame):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data_frame['token'].values)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    encoded_doc = tokenizer.texts_to_sequences(data_frame['token'].values)\n",
    "    max_length = 1\n",
    "    padded_docs = pad_sequences(encoded_doc, maxlen=max_length, padding='post')\n",
    "    return padded_docs, vocab_size, tokenizer\n",
    "\n",
    "# train set\n",
    "train_padded_docs, train_vocab_size, train_tokenizer = tokenize_and_pad(train_frame)\n",
    "\n",
    "# val set\n",
    "val_padded_docs, val_vocab_size, val_tokenizer = tokenize_and_pad(val_frame)\n",
    "\n",
    "# test set\n",
    "test_padded_docs, test_vocab_size, test_tokenizer = tokenize_and_pad(test_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create embedding matrix\n",
    "First, the pretrained glove-embedding has been downloaded from https://nlp.stanford.edu/projects/glove/ and converted to a dictionary format (100-dimensional embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:27, 14294.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in tqdm(f):\n",
    "        values = line.split()\n",
    "        embeddings_index[values[0]] = asarray(values[1:], dtype='float32')\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to create an embedding matrix from a given vocabulary, using the previously loaded glove-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(tokenizer, vocab_size):\n",
    "    # load embedding into memory, skip first\n",
    "    embedding_matrix = zeros((vocab_size, 100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            # print(\"Not in Vocab\", word)\n",
    "            pass\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and train the model\n",
    "## Encode the POS-labels\n",
    "We also need to convert the POS labels to a numerical representation (one-hot-encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(data_frame, trained_encoder = None):\n",
    "    if trained_encoder is None:\n",
    "        trained_encoder = LabelEncoder()\n",
    "        trained_encoder.fit(data_frame['pos'].values)\n",
    "\n",
    "    encoded_Y = trained_encoder.transform(data_frame['pos'].values)\n",
    "    # convert integers to dummy variables (i.e. one hot encoded) -> labels\n",
    "    labels = np_utils.to_categorical(encoded_Y)\n",
    "    return labels, trained_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the structure and layers of the used model\n",
    "Here, a sequential model is used, that receives tokens using an Embedding layer (embedding with the loaded glove-embeddings).  \n",
    "The recurrent structure is implemented using a LSTM layer with 128-units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embedding_matrix, plot_model=False, recurrent_layer='lstm'):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input as Embeddings\n",
    "    model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=1, trainable=False))\n",
    "    # A Bidirectional recurrent layer (LSTM units)\n",
    "    \n",
    "    if recurrent_layer == 'lstm':\n",
    "        model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=128), input_shape=(None, 100)))\n",
    "    \n",
    "    # Dense layer to fit output to label-vector-size\n",
    "    model.add(Dense(45, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    if plot_model:\n",
    "        keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inference(model, tokenizer, vocab_size, test_padded_docs, test_frame):\n",
    "#     # predict the model\n",
    "#     yhat = model.predictd(test_padded_docs, verbose=1)\n",
    "#     # map predicted labels to words\n",
    "#     predicted_labels = []\n",
    "#     for i in yhat:\n",
    "#         for word, index in tokenizer.word_index.items():\n",
    "#             if index == i:\n",
    "#                 predicted_labels.append(word)\n",
    "#                 break\n",
    "#     # map actual labels to words\n",
    "#     actual_labels = []\n",
    "#     for i in test_frame['pos'].values:\n",
    "#         for word, index in tokenizer.word_index.items():\n",
    "#             if index == i:\n",
    "#                 actual_labels.append(word)\n",
    "#                 break\n",
    "#     # create confusion matrix\n",
    "#     confusion_matrix = pd.crosstab(pd.Series(actual_labels), pd.Series(predicted_labels), rownames=['Actual'], colnames=['Predicted'])\n",
    "#     print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the labels of the train data using a one hot encoding. Use the same encoder to create the label vector for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding_matrix = create_embedding_matrix(train_tokenizer, train_vocab_size)\n",
    "train_labels, train_encoder = encode_labels(train_frame)\n",
    "\n",
    "val_labels, _ = encode_labels(val_frame, trained_encoder=train_encoder)\n",
    "test_labels, _ = encode_labels(test_frame, trained_encoder=train_encoder)\n",
    "\n",
    "assert train_labels.shape[1] == val_labels.shape[1] and train_labels.shape[1] == test_labels.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train routine using a bidirectional LSTM layern with 128 units  \n",
    "Evaluate the models training progress using the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 1, 100)            690300    \n",
      "                                                                 \n",
      " bidirectional_10 (Bidirecti  (None, 256)              234496    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 45)                11565     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 936,361\n",
      "Trainable params: 246,061\n",
      "Non-trainable params: 690,300\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1480/1480 [==============================] - 25s 11ms/step - loss: 1.0104 - accuracy: 0.7210 - val_loss: 5.2466 - val_accuracy: 0.2419\n",
      "Epoch 2/10\n",
      "1480/1480 [==============================] - 15s 10ms/step - loss: 0.6012 - accuracy: 0.8076 - val_loss: 5.7338 - val_accuracy: 0.2378\n",
      "Epoch 3/10\n",
      "1480/1480 [==============================] - 14s 9ms/step - loss: 0.5318 - accuracy: 0.8234 - val_loss: 6.0252 - val_accuracy: 0.2381\n",
      "Epoch 4/10\n",
      "1480/1480 [==============================] - 14s 9ms/step - loss: 0.4927 - accuracy: 0.8322 - val_loss: 6.2521 - val_accuracy: 0.2389\n",
      "Epoch 5/10\n",
      "1480/1480 [==============================] - 14s 10ms/step - loss: 0.4672 - accuracy: 0.8375 - val_loss: 6.4497 - val_accuracy: 0.2383\n",
      "Epoch 6/10\n",
      "1480/1480 [==============================] - 14s 9ms/step - loss: 0.4504 - accuracy: 0.8397 - val_loss: 6.6954 - val_accuracy: 0.2431\n",
      "Epoch 7/10\n",
      "1480/1480 [==============================] - 14s 9ms/step - loss: 0.4368 - accuracy: 0.8431 - val_loss: 6.7120 - val_accuracy: 0.2385\n",
      "Epoch 8/10\n",
      "1480/1480 [==============================] - 13s 9ms/step - loss: 0.4269 - accuracy: 0.8446 - val_loss: 6.7625 - val_accuracy: 0.2321\n",
      "Epoch 9/10\n",
      "1480/1480 [==============================] - 13s 9ms/step - loss: 0.4187 - accuracy: 0.8451 - val_loss: 6.9291 - val_accuracy: 0.2397\n",
      "Epoch 10/10\n",
      "1475/1480 [============================>.] - ETA: 0s - loss: 0.4114 - accuracy: 0.8475"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [114], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[39m=\u001b[39m create_model(train_vocab_size, train_embedding_matrix)\n\u001b[0;32m      7\u001b[0m \u001b[39m# fit the model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mtrain_padded_docs, y\u001b[39m=\u001b[39;49mtrain_labels, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(val_padded_docs, val_labels), verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[csv_logger])\n",
      "File \u001b[1;32mc:\\Users\\michi\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\michi\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py:1445\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1431\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1432\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[0;32m   1433\u001b[0m       x\u001b[39m=\u001b[39mval_x,\n\u001b[0;32m   1434\u001b[0m       y\u001b[39m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1443\u001b[0m       model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[0;32m   1444\u001b[0m       steps_per_execution\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution)\n\u001b[1;32m-> 1445\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[0;32m   1446\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[0;32m   1447\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[0;32m   1448\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[0;32m   1449\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[0;32m   1450\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   1451\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1452\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1453\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1454\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1455\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1456\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1457\u001b[0m val_logs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m   1458\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mc:\\Users\\michi\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\michi\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py:1753\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1751\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_metrics()\n\u001b[0;32m   1752\u001b[0m \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m-> 1753\u001b[0m   \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   1754\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m, step_num\u001b[39m=\u001b[39mstep, _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1755\u001b[0m       callbacks\u001b[39m.\u001b[39mon_test_batch_begin(step)\n",
      "File \u001b[1;32mc:\\Users\\michi\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\data_adapter.py:1248\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m   \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1248\u001b[0m original_spe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m   1249\u001b[0m can_run_full_execution \u001b[39m=\u001b[39m (\n\u001b[0;32m   1250\u001b[0m     original_spe \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1252\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\n\u001b[0;32m   1253\u001b[0m     original_spe)\n\u001b[0;32m   1255\u001b[0m \u001b[39mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[1;32mc:\\Users\\michi\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:637\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    636\u001b[0m   \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 637\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_value()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    638\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    639\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\michi\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:715\u001b[0m, in \u001b[0;36mBaseResourceVariable.read_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    712\u001b[0m   value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_variable_op()\n\u001b[0;32m    713\u001b[0m \u001b[39m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m \u001b[39mreturn\u001b[39;00m array_ops\u001b[39m.\u001b[39;49midentity(value)\n",
      "File \u001b[1;32mc:\\Users\\michi\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\michi\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\michi\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:295\u001b[0m, in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39minput\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgraph\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    292\u001b[0m   \u001b[39m# Make sure we get an input with handle data attached from resource\u001b[39;00m\n\u001b[0;32m    293\u001b[0m   \u001b[39m# variables. Variables have correct handle data when graph building.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m   \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(\u001b[39minput\u001b[39m)\n\u001b[1;32m--> 295\u001b[0m ret \u001b[39m=\u001b[39m gen_array_ops\u001b[39m.\u001b[39;49midentity(\u001b[39minput\u001b[39;49m, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m    296\u001b[0m \u001b[39m# Propagate handle data for happier shape inference for resource variables.\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39minput\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_handle_data\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\michi\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:4063\u001b[0m, in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   4061\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   4062\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 4063\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   4064\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mIdentity\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, \u001b[39minput\u001b[39;49m)\n\u001b[0;32m   4065\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   4066\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('log/log_lstm_128.csv', append=False, separator=';')\n",
    "\n",
    "model = create_model(train_vocab_size, train_embedding_matrix)\n",
    "\n",
    "# fit the model\n",
    "model.fit(x=train_padded_docs, y=train_labels, epochs=10, validation_data=(val_padded_docs, val_labels), verbose=1, callbacks=[csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1480/1480 [==============================] - 6s 4ms/step - loss: 0.3939 - accuracy: 0.8550\n",
      "975/975 [==============================] - 4s 4ms/step - loss: 6.9771 - accuracy: 0.2416\n",
      "486/486 [==============================] - 2s 4ms/step - loss: 6.9168 - accuracy: 0.2484\n",
      "Train Accuracy: 85.50 %\n",
      "Test Accuracy: 24.84 %\n",
      "Validation Accuracy: 24.16 %\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "train_loss, train_accuracy = model.evaluate(train_padded_docs, train_labels, verbose=1)\n",
    "val_loss, val_accuracy = model.evaluate(val_padded_docs, val_labels, verbose=1)\n",
    "test_loss, test_accuracy = model.evaluate(test_padded_docs, test_labels, verbose=1)\n",
    "\n",
    "print('Train Accuracy: {n:.2f} %'.format(n=(train_accuracy * 100)))\n",
    "print('Test Accuracy: {n:.2f} %'.format(n=(test_accuracy * 100)))\n",
    "print('Validation Accuracy: {n:.2f} %'.format(n=(val_accuracy * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'categorical_crossentropy/softmax_cross_entropy_with_logits' defined at (most recent call last):\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/qr/scdb4s4n6dz5463spyd32yth0000gn/T/ipykernel_21772/2052202248.py\", line 2, in <module>\n      val_loss, val_accuracy = model.evaluate(val_padded_docs, val_labels, verbose=1)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1947, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1727, in test_function\n      return step_function(self, iterator)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1713, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1701, in run_step\n      outputs = model.test_step(data)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1667, in test_step\n      self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/losses.py\", line 1990, in categorical_crossentropy\n      return backend.categorical_crossentropy(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/backend.py\", line 5535, in categorical_crossentropy\n      return tf.nn.softmax_cross_entropy_with_logits(\nNode: 'categorical_crossentropy/softmax_cross_entropy_with_logits'\nlogits and labels must be broadcastable: logits_size=[32,44] labels_size=[32,43]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_test_function_255175]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m val_embedding_matrix \u001b[39m=\u001b[39m create_embedding_matrix(val_tokenizer, val_vocab_size)\n\u001b[1;32m      3\u001b[0m val_labels, val_encoder \u001b[39m=\u001b[39m encode_labels(val_frame)\n\u001b[0;32m----> 5\u001b[0m loss, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mevaluate(val_padded_docs, val_labels, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/play/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/play/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'categorical_crossentropy/softmax_cross_entropy_with_logits' defined at (most recent call last):\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/qr/scdb4s4n6dz5463spyd32yth0000gn/T/ipykernel_21772/2052202248.py\", line 2, in <module>\n      val_loss, val_accuracy = model.evaluate(val_padded_docs, val_labels, verbose=1)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1947, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1727, in test_function\n      return step_function(self, iterator)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1713, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1701, in run_step\n      outputs = model.test_step(data)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1667, in test_step\n      self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/losses.py\", line 1990, in categorical_crossentropy\n      return backend.categorical_crossentropy(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/backend.py\", line 5535, in categorical_crossentropy\n      return tf.nn.softmax_cross_entropy_with_logits(\nNode: 'categorical_crossentropy/softmax_cross_entropy_with_logits'\nlogits and labels must be broadcastable: logits_size=[32,44] labels_size=[32,43]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_test_function_255175]"
     ]
    }
   ],
   "source": [
    "# validate model\n",
    "val_embedding_matrix = create_embedding_matrix(val_tokenizer, val_vocab_size)\n",
    "val_labels, val_encoder = encode_labels(val_frame)\n",
    "\n",
    "loss, accuracy = model.evaluate(val_padded_docs, val_labels, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879/879 [==============================] - 1s 613us/step\n",
      "[[1.0907109e-05 2.2433024e-04 2.1703347e-06 ... 2.8716912e-08\n",
      "  4.9551250e-07 7.0266455e-05]\n",
      " [4.2851013e-08 1.0514050e-06 4.9462809e-08 ... 8.0239711e-11\n",
      "  5.8290900e-11 1.2585730e-07]\n",
      " [2.6137733e-03 6.1667468e-02 2.5079817e-06 ... 2.7957765e-07\n",
      "  1.7772708e-06 5.3486343e-02]\n",
      " ...\n",
      " [1.1580768e-07 3.0247747e-06 7.4308076e-08 ... 2.9441841e-11\n",
      "  2.2790166e-10 4.1862222e-07]\n",
      " [4.4803182e-05 1.6333588e-04 4.8778595e-05 ... 6.6113500e-08\n",
      "  4.7637050e-07 6.0173814e-05]\n",
      " [2.6137733e-03 6.1667468e-02 2.5079817e-06 ... 2.7957765e-07\n",
      "  1.7772708e-06 5.3486343e-02]]\n"
     ]
    }
   ],
   "source": [
    "# predict model val\n",
    "predictions = model.predict(val_padded_docs, verbose=1)\n",
    "\n",
    "#TODO: compare\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST AREA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# # predict model test\n",
    "# predictions = model.predict(test_padded_docs, verbose=1)\n",
    "# # map predicted labels to words\n",
    "# predicted_labels = []\n",
    "# for i in predictions:\n",
    "#     for word, index in test_tokenizer.word_index.items():\n",
    "#         if index == np.argmax(i):\n",
    "#             predicted_labels.append(word)\n",
    "#             break\n",
    "# # map actual labels to words\n",
    "# actual_labels = []\n",
    "# for i in test_frame['pos'].values:\n",
    "#     for word, index in test_tokenizer.word_index.items():\n",
    "#         if index == i:\n",
    "#             actual_labels.append(word)\n",
    "#             break\n",
    "# # create confusion matrix\n",
    "# confusion_matrix = pd.crosstab(pd.Series(actual_labels), pd.Series(predicted_labels), rownames=['Actual'], colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# np.argmax(val_labels[0])\n",
    "# np.argmax(predictions[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88bc5bd50aef891d31dedec3c0c74b50329290996ba08fef3dc094ea24ed9616"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
