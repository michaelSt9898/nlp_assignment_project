{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Download the dataset from https://www.kaggle.com/c/word2vec-nlp-tutorial/data\n",
    "#TODO: Unzip the dataset and place it in the same folder as this notebook\n",
    "#TODO: Change the path to the dataset below\n",
    "# dataset_path = Path(os.getcwd()) / 'data' / 'labeledTrainData.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and extract the dataset\n",
    "The dataset is downloaded from the provided URL and extracted into the 'Dataset' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful download\n",
      "Extracting dataset\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
    "\n",
    "dataset_folder = './Dataset'\n",
    "if not os.path.exists(dataset_folder):\n",
    "    os.mkdir(dataset_folder)\n",
    "\n",
    "dataset_path = os.path.join(dataset_folder, 'dependency_treebank.zip')\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    urllib.request.urlretrieve(url, dataset_path)\n",
    "    print(\"Successful download\")    \n",
    "    with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
    "        print(\"Extracting dataset\")\n",
    "        zip_ref.extractall(dataset_folder+'/dependency_treebank')\n",
    "    os.remove(dataset_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data\n",
    "The data (199 samples in total) is split into a train, validation and test set:\n",
    "- 100 train samples\n",
    "- 50 validation samples\n",
    "- 49 test samples \n",
    "The sets are stored in data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split size\n",
    "TRAIN_SPLIT = 100\n",
    "VAL_SPLIT = 150\n",
    "\n",
    "# Define file iterator\n",
    "def file_iterator():\n",
    "    data_dir = Path('dependency_treebank')\n",
    "    for data_file in filter(lambda f: os.path.isfile(data_dir/f) and f.endswith('.dp'), os.listdir(data_dir)):\n",
    "        yield data_dir/data_file\n",
    "\n",
    "# Create train, val and test set\n",
    "train_set = []\n",
    "test_set = []\n",
    "val_set = []\n",
    "file_counter = 0\n",
    "\n",
    "# Iterate over files and perform split\n",
    "for file in file_iterator():\n",
    "    file_counter += 1\n",
    "    if file_counter <= TRAIN_SPLIT:\n",
    "        train_set.append(pd.read_csv(file, sep=\"\\t\", names=['token', 'pos'], usecols=[0, 1], engine='python'))\n",
    "    elif file_counter <= VAL_SPLIT:\n",
    "        val_set.append(pd.read_csv(file, sep=\"\\t\", names=['token', 'pos'], usecols=[0, 1], engine='python'))\n",
    "    else:\n",
    "        test_set.append(pd.read_csv(file, sep=\"\\t\", names=['token', 'pos'], usecols=[0, 1], engine='python'))\n",
    "\n",
    "# Check for correct split\n",
    "assert len(train_set) == 100\n",
    "assert len(val_set) == 50\n",
    "assert len(test_set) == 49\n",
    "\n",
    "# Change to pandas dataframe\n",
    "train_frame = pd.concat(train_set)\n",
    "test_frame = pd.concat(test_set)\n",
    "val_frame = pd.concat(val_set)\n",
    "\n",
    "# Check for correct transformation\n",
    "assert sum([e.shape[0] for e in train_set]) == train_frame.shape[0]\n",
    "assert sum([e.shape[0] for e in test_set]) == test_frame.shape[0]\n",
    "assert sum([e.shape[0] for e in val_set]) == val_frame.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the structure of the constructed data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pierre</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vinken</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>years</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    token  pos\n",
       "0  Pierre  NNP\n",
       "1  Vinken  NNP\n",
       "2       ,    ,\n",
       "3      61   CD\n",
       "4   years  NNS"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intelogic</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trace</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inc.</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>San</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token  pos\n",
       "0  Intelogic  NNP\n",
       "1      Trace  NNP\n",
       "2       Inc.  NNP\n",
       "3          ,    ,\n",
       "4        San  NNP"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>House-Senate</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conference</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>approved</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>major</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          token  pos\n",
       "0             A   DT\n",
       "1  House-Senate  NNP\n",
       "2    conference   NN\n",
       "3      approved  VBD\n",
       "4         major   JJ"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and padding\n",
    "The data frames are now tokenized and additional padded to have a unit input length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine to tokenize and pad data to unit length\n",
    "def tokenize_and_pad(data_frame):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data_frame['token'].values)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    encoded_doc = tokenizer.texts_to_sequences(data_frame['token'].values)\n",
    "    max_length = 1\n",
    "    padded_docs = pad_sequences(encoded_doc, maxlen=max_length, padding='post')\n",
    "    return padded_docs, vocab_size, tokenizer\n",
    "\n",
    "# train set\n",
    "train_padded_docs, train_vocab_size, train_tokenizer = tokenize_and_pad(train_frame)\n",
    "\n",
    "# val set\n",
    "val_padded_docs, val_vocab_size, val_tokenizer = tokenize_and_pad(val_frame)\n",
    "\n",
    "# test set\n",
    "test_padded_docs, test_vocab_size, test_tokenizer = tokenize_and_pad(test_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create embedding matrix\n",
    "First, the pretrained glove-embedding has been downloaded from https://nlp.stanford.edu/projects/glove/ and converted to a dictionary format (100-dimensional embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "with open('glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        embeddings_index[values[0]] = asarray(values[1:], dtype='float32')\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to create an embedding matrix from a given vocabulary, using the previously loaded glove-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(tokenizer, vocab_size):\n",
    "    # load embedding into memory, skip first\n",
    "    embedding_matrix = zeros((vocab_size, 100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            # print(\"Not in Vocab\", word)\n",
    "            pass\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and train the model\n",
    "## Encode the POS-labels\n",
    "We also need to convert the POS labels to a numerical representation (one-hot-encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(data_frame):\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(data_frame['pos'].values)\n",
    "    encoded_Y = encoder.transform(data_frame['pos'].values)\n",
    "    # convert integers to dummy variables (i.e. one hot encoded) -> labels\n",
    "    labels = np_utils.to_categorical(encoded_Y)\n",
    "    return labels, encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the structure and layers of the used model\n",
    "Here, a sequential model is used, that receives tokens using an Embedding layer (embedding with the loaded glove-embeddings).  \n",
    "The recurrent structure is implemented using a LSTM layer with 128-units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embedding_matrix, plot_model=False):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input as Embeddings\n",
    "    model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=1, trainable=False))\n",
    "    # A Bidirectional recurrent layer (LSTM units)\n",
    "    model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=128), input_shape=(None, 50)))\n",
    "    # Dense layer to fit output to label-vector-size\n",
    "    model.add(Dense(44, activation='softmax'))\n",
    "\n",
    "    #TODO: Put in train function\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    if plot_model:\n",
    "        keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
    "    return model\n",
    "\n",
    "# run the model\n",
    "def train_model(model, padded_docs, labels):\n",
    "    # fit the model\n",
    "    model.fit(padded_docs, labels, epochs=20, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(padded_docs, labels, verbose=1)\n",
    "    print('Accuracy: %f' % (accuracy * 100))\n",
    "\n",
    "def inference(model, tokenizer, vocab_size, test_padded_docs, test_frame):\n",
    "    # predict the model\n",
    "    yhat = model.predict(test_padded_docs, verbose=1)\n",
    "    # map predicted labels to words\n",
    "    predicted_labels = []\n",
    "    for i in yhat:\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == i:\n",
    "                predicted_labels.append(word)\n",
    "                break\n",
    "    # map actual labels to words\n",
    "    actual_labels = []\n",
    "    for i in test_frame['pos'].values:\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == i:\n",
    "                actual_labels.append(word)\n",
    "                break\n",
    "    # create confusion matrix\n",
    "    confusion_matrix = pd.crosstab(pd.Series(actual_labels), pd.Series(predicted_labels), rownames=['Actual'], colnames=['Predicted'])\n",
    "    print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train routine using a bidirectional LSTM layern with 128 units  \n",
    "Evaluate the models training progress using the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding_matrix = create_embedding_matrix(train_tokenizer, train_vocab_size)\n",
    "train_labels, train_encoder = encode_labels(train_frame)\n",
    "model = create_model(train_vocab_size, train_embedding_matrix)\n",
    "\n",
    "# fit the model\n",
    "model.fit(train_padded_docs, train_labels, epochs=20, verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(train_padded_docs, train_labels, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879/879 [==============================] - 2s 904us/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [38], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m val_embedding_matrix \u001b[39m=\u001b[39m create_embedding_matrix(val_tokenizer, val_vocab_size)\n\u001b[1;32m      3\u001b[0m val_labels, val_encoder \u001b[39m=\u001b[39m encode_labels(val_frame)\n\u001b[0;32m----> 4\u001b[0m inference(model, val_tokenizer, val_vocab_size, val_padded_docs, val_frame)\n",
      "Cell \u001b[0;32mIn [37], line 36\u001b[0m, in \u001b[0;36minference\u001b[0;34m(model, tokenizer, vocab_size, test_padded_docs, test_frame)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m yhat:\n\u001b[1;32m     35\u001b[0m     \u001b[39mfor\u001b[39;00m word, index \u001b[39min\u001b[39;00m tokenizer\u001b[39m.\u001b[39mword_index\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> 36\u001b[0m         \u001b[39mif\u001b[39;00m index \u001b[39m==\u001b[39m i:\n\u001b[1;32m     37\u001b[0m             predicted_labels\u001b[39m.\u001b[39mappend(word)\n\u001b[1;32m     38\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "val_embedding_matrix = create_embedding_matrix(val_tokenizer, val_vocab_size)\n",
    "val_labels, val_encoder = encode_labels(val_frame)\n",
    "inference(model, val_tokenizer, val_vocab_size, val_padded_docs, val_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'categorical_crossentropy/softmax_cross_entropy_with_logits' defined at (most recent call last):\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/qr/scdb4s4n6dz5463spyd32yth0000gn/T/ipykernel_21772/2052202248.py\", line 2, in <module>\n      val_loss, val_accuracy = model.evaluate(val_padded_docs, val_labels, verbose=1)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1947, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1727, in test_function\n      return step_function(self, iterator)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1713, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1701, in run_step\n      outputs = model.test_step(data)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1667, in test_step\n      self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/losses.py\", line 1990, in categorical_crossentropy\n      return backend.categorical_crossentropy(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/backend.py\", line 5535, in categorical_crossentropy\n      return tf.nn.softmax_cross_entropy_with_logits(\nNode: 'categorical_crossentropy/softmax_cross_entropy_with_logits'\nlogits and labels must be broadcastable: logits_size=[32,44] labels_size=[32,43]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_test_function_255175]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m val_embedding_matrix \u001b[39m=\u001b[39m create_embedding_matrix(val_tokenizer, val_vocab_size)\n\u001b[1;32m      3\u001b[0m val_labels, val_encoder \u001b[39m=\u001b[39m encode_labels(val_frame)\n\u001b[0;32m----> 5\u001b[0m loss, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mevaluate(val_padded_docs, val_labels, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/play/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/play/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'categorical_crossentropy/softmax_cross_entropy_with_logits' defined at (most recent call last):\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/qr/scdb4s4n6dz5463spyd32yth0000gn/T/ipykernel_21772/2052202248.py\", line 2, in <module>\n      val_loss, val_accuracy = model.evaluate(val_padded_docs, val_labels, verbose=1)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1947, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1727, in test_function\n      return step_function(self, iterator)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1713, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1701, in run_step\n      outputs = model.test_step(data)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1667, in test_step\n      self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/losses.py\", line 1990, in categorical_crossentropy\n      return backend.categorical_crossentropy(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/backend.py\", line 5535, in categorical_crossentropy\n      return tf.nn.softmax_cross_entropy_with_logits(\nNode: 'categorical_crossentropy/softmax_cross_entropy_with_logits'\nlogits and labels must be broadcastable: logits_size=[32,44] labels_size=[32,43]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_test_function_255175]"
     ]
    }
   ],
   "source": [
    "# validate model\n",
    "val_embedding_matrix = create_embedding_matrix(val_tokenizer, val_vocab_size)\n",
    "val_labels, val_encoder = encode_labels(val_frame)\n",
    "\n",
    "loss, accuracy = model.evaluate(val_padded_docs, val_labels, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879/879 [==============================] - 1s 613us/step\n",
      "[[1.0907109e-05 2.2433024e-04 2.1703347e-06 ... 2.8716912e-08\n",
      "  4.9551250e-07 7.0266455e-05]\n",
      " [4.2851013e-08 1.0514050e-06 4.9462809e-08 ... 8.0239711e-11\n",
      "  5.8290900e-11 1.2585730e-07]\n",
      " [2.6137733e-03 6.1667468e-02 2.5079817e-06 ... 2.7957765e-07\n",
      "  1.7772708e-06 5.3486343e-02]\n",
      " ...\n",
      " [1.1580768e-07 3.0247747e-06 7.4308076e-08 ... 2.9441841e-11\n",
      "  2.2790166e-10 4.1862222e-07]\n",
      " [4.4803182e-05 1.6333588e-04 4.8778595e-05 ... 6.6113500e-08\n",
      "  4.7637050e-07 6.0173814e-05]\n",
      " [2.6137733e-03 6.1667468e-02 2.5079817e-06 ... 2.7957765e-07\n",
      "  1.7772708e-06 5.3486343e-02]]\n"
     ]
    }
   ],
   "source": [
    "# predict model val\n",
    "predictions = model.predict(val_padded_docs, verbose=1)\n",
    "\n",
    "#TODO: compare\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST AREA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# # predict model test\n",
    "# predictions = model.predict(test_padded_docs, verbose=1)\n",
    "# # map predicted labels to words\n",
    "# predicted_labels = []\n",
    "# for i in predictions:\n",
    "#     for word, index in test_tokenizer.word_index.items():\n",
    "#         if index == np.argmax(i):\n",
    "#             predicted_labels.append(word)\n",
    "#             break\n",
    "# # map actual labels to words\n",
    "# actual_labels = []\n",
    "# for i in test_frame['pos'].values:\n",
    "#     for word, index in test_tokenizer.word_index.items():\n",
    "#         if index == i:\n",
    "#             actual_labels.append(word)\n",
    "#             break\n",
    "# # create confusion matrix\n",
    "# confusion_matrix = pd.crosstab(pd.Series(actual_labels), pd.Series(predicted_labels), rownames=['Actual'], colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# np.argmax(val_labels[0])\n",
    "# np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "model.add(Dense(16, activation = 'relu', input_dim = 243))\n",
    "# Adding the output layer\n",
    "model.add(Dense(units = 1))\n",
    "model.compile(optimizer = 'adam',loss = 'mean_squared_error')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=40) # Model stop training after 40 epoch where validation loss didnt decrease\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True) #You save model weight at the epoch where validation loss is minimal\n",
    "train = model.fit((train_X, train_label, batch_size=batch_size),epochs=1000,verbose=1,validation_data=(valid_X, valid_label),callbacks=[es,mc])#you can run for 1000 epoch btw model will stop after 40 epoch without better validation loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88bc5bd50aef891d31dedec3c0c74b50329290996ba08fef3dc094ea24ed9616"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
