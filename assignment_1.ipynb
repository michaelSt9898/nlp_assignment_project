{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Download the dataset from https://www.kaggle.com/c/word2vec-nlp-tutorial/data\n",
    "#TODO: Unzip the dataset and place it in the same folder as this notebook\n",
    "#TODO: Change the path to the dataset below\n",
    "# dataset_path = Path(os.getcwd()) / 'data' / 'labeledTrainData.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and extract the dataset\n",
    "The dataset is downloaded from the provided URL and extracted into the 'Dataset' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful download\n",
      "Extracting dataset\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
    "\n",
    "dataset_folder = './Dataset'\n",
    "if not os.path.exists(dataset_folder):\n",
    "    os.mkdir(dataset_folder)\n",
    "\n",
    "dataset_path = os.path.join(dataset_folder, 'dependency_treebank.zip')\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    urllib.request.urlretrieve(url, dataset_path)\n",
    "    print(\"Successful download\")    \n",
    "    with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
    "        print(\"Extracting dataset\")\n",
    "        zip_ref.extractall(dataset_folder+'/dependency_treebank')\n",
    "    os.remove(dataset_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data\n",
    "The data (199 samples in total) is split into a train, validation and test set:\n",
    "- 100 train samples\n",
    "- 50 validation samples\n",
    "- 49 test samples \n",
    "The sets are stored in data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split size\n",
    "TRAIN_SPLIT = 100\n",
    "VAL_SPLIT = 150\n",
    "\n",
    "# Define file iterator\n",
    "def file_iterator():\n",
    "    data_dir = Path('dependency_treebank')\n",
    "    for data_file in filter(lambda f: os.path.isfile(data_dir/f) and f.endswith('.dp'), os.listdir(data_dir)):\n",
    "        yield data_dir/data_file\n",
    "\n",
    "# Create train, val and test set\n",
    "train_set = []\n",
    "test_set = []\n",
    "val_set = []\n",
    "file_counter = 0\n",
    "\n",
    "# Iterate over files and perform split\n",
    "for file in file_iterator():\n",
    "    file_counter += 1\n",
    "    if file_counter <= TRAIN_SPLIT:\n",
    "        train_set.append(pd.read_csv(file, sep=\"\\t\", names=['token', 'pos'], usecols=[0, 1], engine='python'))\n",
    "    elif file_counter <= VAL_SPLIT:\n",
    "        val_set.append(pd.read_csv(file, sep=\"\\t\", names=['token', 'pos'], usecols=[0, 1], engine='python'))\n",
    "    else:\n",
    "        test_set.append(pd.read_csv(file, sep=\"\\t\", names=['token', 'pos'], usecols=[0, 1], engine='python'))\n",
    "\n",
    "# Check for correct split\n",
    "assert len(train_set) == 100\n",
    "assert len(val_set) == 50\n",
    "assert len(test_set) == 49\n",
    "\n",
    "# Change to pandas dataframe\n",
    "train_frame = pd.concat(train_set)\n",
    "test_frame = pd.concat(test_set)\n",
    "val_frame = pd.concat(val_set)\n",
    "\n",
    "# Check for correct transformation\n",
    "assert sum([e.shape[0] for e in train_set]) == train_frame.shape[0]\n",
    "assert sum([e.shape[0] for e in test_set]) == test_frame.shape[0]\n",
    "assert sum([e.shape[0] for e in val_set]) == val_frame.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the structure of the constructed data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reference</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>your</td>\n",
       "      <td>PRP$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oct.</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token   pos\n",
       "0         In    IN\n",
       "1  reference    NN\n",
       "2         to    TO\n",
       "3       your  PRP$\n",
       "4       Oct.   NNP"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>six</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>years</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T.</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token  pos\n",
       "0    For   IN\n",
       "1    six   CD\n",
       "2  years  NNS\n",
       "3      ,    ,\n",
       "4     T.  NNP"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lord</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chilver</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63-year-old</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chairman</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         token  pos\n",
       "0         Lord  NNP\n",
       "1      Chilver  NNP\n",
       "2            ,    ,\n",
       "3  63-year-old   JJ\n",
       "4     chairman   NN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and padding\n",
    "The data frames are now tokenized and additional padded to have a unit input length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 8424\n",
      "Vocabulary size: 8424\n",
      "Vocabulary size: 8424\n"
     ]
    }
   ],
   "source": [
    "# merge train_frame['token'].values and val_frame['token'].values\n",
    "new_train_frame = pd.concat([train_frame, val_frame], ignore_index=True)\n",
    "\n",
    "# create tokenizer and fit on training and val set\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(new_train_frame['token'].values)\n",
    "\n",
    "# Routine to tokenize and pad data to unit length\n",
    "def tokenize_and_pad(data_frame, tokenizer):\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print('Vocabulary size: %d' % vocab_size)\n",
    "    encoded_doc = tokenizer.texts_to_sequences(data_frame['token'].values)\n",
    "    max_length = max([len(s.split()) for s in data_frame['token'].values]) # Changed max length to max length of a sentence\n",
    "    padded_docs = pad_sequences(encoded_doc, maxlen=max_length, padding='post')\n",
    "    return padded_docs, vocab_size, tokenizer\n",
    "\n",
    "# train set\n",
    "train_padded_docs, train_vocab_size, train_tokenizer = tokenize_and_pad(train_frame, tokenizer)\n",
    "\n",
    "# val set\n",
    "val_padded_docs, val_vocab_size, val_tokenizer = tokenize_and_pad(val_frame, tokenizer)\n",
    "\n",
    "# test set\n",
    "test_padded_docs, test_vocab_size, test_tokenizer = tokenize_and_pad(test_frame, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create embedding matrix\n",
    "First, the pretrained glove-embedding has been downloaded from https://nlp.stanford.edu/projects/glove/ and converted to a dictionary format (100-dimensional embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "with open('glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        embeddings_index[values[0]] = asarray(values[1:], dtype='float32')\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to create an embedding matrix from a given vocabulary, using the previously loaded glove-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(tokenizer, vocab_size):\n",
    "    # load embedding into memory, skip first\n",
    "    embedding_matrix = zeros((vocab_size, 100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            # print(\"Not in Vocab\", word)\n",
    "            pass\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and train the model\n",
    "## Encode the POS-labels\n",
    "We also need to convert the POS labels to a numerical representation (one-hot-encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(data_frame):\n",
    "    # combined_frame = pd.concat([data_frame, train_val_frame], ignore_index=True)\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(new_train_frame['pos'].values)\n",
    "    encoded_Y = encoder.transform(data_frame['pos'].values)\n",
    "    # convert integers to dummy variables (i.e. one hot encoded) -> labels\n",
    "    labels = np_utils.to_categorical(encoded_Y)\n",
    "    return labels, encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the structure and layers of the used model\n",
    "Here, a sequential model is used, that receives tokens using an Embedding layer (embedding with the loaded glove-embeddings).  \n",
    "The recurrent structure is implemented using a LSTM layer with 128-units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embedding_matrix, plot_model=False):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input as Embeddings\n",
    "    model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=1, trainable=False))\n",
    "    # A Bidirectional recurrent layer (LSTM units)\n",
    "    model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=128), input_shape=(None, 50)))\n",
    "    # Dense layer to fit output to label-vector-size\n",
    "    model.add(Dense(45, activation='softmax'))\n",
    "\n",
    "    #TODO: Put in train function\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    if plot_model:\n",
    "        keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
    "    return model\n",
    "\n",
    "# run the model\n",
    "def train_model(model, padded_docs, labels):\n",
    "    # fit the model\n",
    "    model.fit(padded_docs, labels, epochs=20, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(padded_docs, labels, verbose=1)\n",
    "    print('Accuracy: %f' % (accuracy * 100))\n",
    "\n",
    "def inference(model, tokenizer, vocab_size, test_padded_docs, test_frame):\n",
    "    # predict the model\n",
    "    yhat = model.predict(test_padded_docs, verbose=1)\n",
    "    # map predicted labels to words\n",
    "    predicted_labels = []\n",
    "    for i in yhat:\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == i:\n",
    "                predicted_labels.append(word)\n",
    "                break\n",
    "    # map actual labels to words\n",
    "    actual_labels = []\n",
    "    for i in test_frame['pos'].values:\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == i:\n",
    "                actual_labels.append(word)\n",
    "                break\n",
    "    # create confusion matrix\n",
    "    confusion_matrix = pd.crosstab(pd.Series(actual_labels), pd.Series(predicted_labels), rownames=['Actual'], colnames=['Predicted'])\n",
    "    print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train routine using a bidirectional LSTM layern with 128 units  \n",
    "Evaluate the models training progress using the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1, 100)            842400    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 256)              234496    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 45)                11565     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,088,461\n",
      "Trainable params: 246,061\n",
      "Non-trainable params: 842,400\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 14:40:51.709668: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1413/1413 [==============================] - 4s 2ms/step - loss: 0.9948 - accuracy: 0.7264\n",
      "Epoch 2/20\n",
      "1413/1413 [==============================] - 3s 2ms/step - loss: 0.5793 - accuracy: 0.8136\n",
      "Epoch 3/20\n",
      "1413/1413 [==============================] - 3s 2ms/step - loss: 0.5071 - accuracy: 0.8287\n",
      "Epoch 4/20\n",
      "1413/1413 [==============================] - 3s 2ms/step - loss: 0.4681 - accuracy: 0.8381\n",
      "Epoch 5/20\n",
      "1413/1413 [==============================] - 3s 2ms/step - loss: 0.4448 - accuracy: 0.8426\n",
      "Epoch 6/20\n",
      "1413/1413 [==============================] - 3s 2ms/step - loss: 0.4276 - accuracy: 0.8445\n",
      "Epoch 7/20\n",
      "1413/1413 [==============================] - 3s 2ms/step - loss: 0.4154 - accuracy: 0.8472\n",
      "Epoch 8/20\n",
      "1413/1413 [==============================] - 3s 2ms/step - loss: 0.4058 - accuracy: 0.8492\n",
      "Epoch 9/20\n",
      "1413/1413 [==============================] - 3s 2ms/step - loss: 0.3981 - accuracy: 0.8512\n",
      "Epoch 10/20\n",
      "1413/1413 [==============================] - 3s 2ms/step - loss: 0.3929 - accuracy: 0.8525\n",
      "Epoch 11/20\n",
      "1413/1413 [==============================] - 3s 2ms/step - loss: 0.3877 - accuracy: 0.8517\n",
      "Epoch 12/20\n",
      "1413/1413 [==============================] - 3s 2ms/step - loss: 0.3835 - accuracy: 0.8534\n",
      "Epoch 13/20\n",
      "1413/1413 [==============================] - 3s 2ms/step - loss: 0.3798 - accuracy: 0.8526\n",
      "Epoch 14/20\n",
      "1413/1413 [==============================] - 3s 2ms/step - loss: 0.3768 - accuracy: 0.8550\n",
      "Epoch 15/20\n",
      "1413/1413 [==============================] - 3s 2ms/step - loss: 0.3748 - accuracy: 0.8548\n",
      "Epoch 16/20\n",
      "1413/1413 [==============================] - 3s 2ms/step - loss: 0.3729 - accuracy: 0.8537\n",
      "Epoch 17/20\n",
      "1413/1413 [==============================] - 3s 2ms/step - loss: 0.3700 - accuracy: 0.8557\n",
      "Epoch 18/20\n",
      "1413/1413 [==============================] - 3s 2ms/step - loss: 0.3694 - accuracy: 0.8544\n",
      "Epoch 19/20\n",
      "1413/1413 [==============================] - 3s 2ms/step - loss: 0.3668 - accuracy: 0.8567\n",
      "Epoch 20/20\n",
      "1413/1413 [==============================] - 3s 2ms/step - loss: 0.3648 - accuracy: 0.8565\n",
      "1413/1413 [==============================] - 2s 936us/step - loss: 0.3500 - accuracy: 0.8552\n",
      "Accuracy: 85.522443\n"
     ]
    }
   ],
   "source": [
    "train_embedding_matrix = create_embedding_matrix(train_tokenizer, train_vocab_size)\n",
    "train_labels, train_encoder = encode_labels(train_frame)\n",
    "model = create_model(train_vocab_size, train_embedding_matrix)\n",
    "\n",
    "# fit the model\n",
    "model.fit(train_padded_docs, train_labels, epochs=20, verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(train_padded_docs, train_labels, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879/879 [==============================] - 1s 792us/step - loss: 0.5278 - accuracy: 0.8140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.52779221534729, 0.8139940500259399]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels, val_encoder = encode_labels(val_frame)\n",
    "loss, accuracy = model.evaluate(val_padded_docs, val_labels, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST AREA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# # predict model test\n",
    "# predictions = model.predict(test_padded_docs, verbose=1)\n",
    "# # map predicted labels to words\n",
    "# predicted_labels = []\n",
    "# for i in predictions:\n",
    "#     for word, index in test_tokenizer.word_index.items():\n",
    "#         if index == np.argmax(i):\n",
    "#             predicted_labels.append(word)\n",
    "#             break\n",
    "# # map actual labels to words\n",
    "# actual_labels = []\n",
    "# for i in test_frame['pos'].values:\n",
    "#     for word, index in test_tokenizer.word_index.items():\n",
    "#         if index == i:\n",
    "#             actual_labels.append(word)\n",
    "#             break\n",
    "# # create confusion matrix\n",
    "# confusion_matrix = pd.crosstab(pd.Series(actual_labels), pd.Series(predicted_labels), rownames=['Actual'], colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# np.argmax(val_labels[0])\n",
    "# np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "model.add(Dense(16, activation = 'relu', input_dim = 243))\n",
    "# Adding the output layer\n",
    "model.add(Dense(units = 1))\n",
    "model.compile(optimizer = 'adam',loss = 'mean_squared_error')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=40) # Model stop training after 40 epoch where validation loss didnt decrease\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True) #You save model weight at the epoch where validation loss is minimal\n",
    "train = model.fit((train_X, train_label, batch_size=batch_size),epochs=1000,verbose=1,validation_data=(valid_X, valid_label),callbacks=[es,mc])#you can run for 1000 epoch btw model will stop after 40 epoch without better validation loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88bc5bd50aef891d31dedec3c0c74b50329290996ba08fef3dc094ea24ed9616"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
