{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "The following tasks have to be done:\n",
    "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
    "*   Embed the words using GloVe embeddings\n",
    "*   Create a baseline model, using a simple neural architecture\n",
    "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
    "*   Evaluate your two best model\n",
    "*   Analyze the errors of your model\n",
    "## Split the data into the training-, test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to iterate over all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_iterator():\n",
    "    data_dir = Path('dependency_treebank')\n",
    "    for data_file in filter(lambda f: os.path.isfile(data_dir/f) and f.endswith('.dp'), os.listdir(data_dir)):\n",
    "        yield data_dir/data_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the text files using the file iterator and perform data set splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199it [00:00, 557.26it/s]\n"
     ]
    }
   ],
   "source": [
    "train_split = 100\n",
    "val_split = 150\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in tqdm(file_iterator()):\n",
    "    data.append(pd.read_csv(file, sep=\"\\t\", names=['token', 'pos'], usecols=[0, 1], engine='python'))\n",
    "\n",
    "data_frame = pd.concat(data)\n",
    "\n",
    "train_frame = pd.concat(data[:train_split])\n",
    "test_frame = pd.concat(data[train_split:val_split])\n",
    "val_frame = pd.concat(data[val_split:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot-encode the part of speech attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "labels_ohe = ohe.fit_transform(data_frame[['pos']]).toarray()\n",
    "\n",
    "train_labels = list(labels_ohe[:train_frame.shape[0]])\n",
    "test_labels = list(labels_ohe[train_frame.shape[0]:train_frame.shape[0]+test_frame.shape[0]])\n",
    "val_labels = list(labels_ohe[train_frame.shape[0]+test_frame.shape[0]:])\n",
    "\n",
    "train_frame['pos_ohe'] = train_labels\n",
    "test_frame['pos_ohe'] = test_labels\n",
    "val_frame['pos_ohe'] = val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(model_type: str,\n",
    "                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    \"\"\"\n",
    "    Loads a pre-trained word embedding model via gensim library.\n",
    "\n",
    "    :param model_type: name of the word embedding model to load.\n",
    "    :param embedding_dimension: size of the embedding space to consider\n",
    "\n",
    "    :return\n",
    "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
    "    \"\"\"\n",
    "    download_path = \"\"\n",
    "    if model_type.strip().lower() == 'word2vec':\n",
    "        download_path = \"word2vec-google-news-300\"\n",
    "\n",
    "    elif model_type.strip().lower() == 'glove':\n",
    "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    elif model_type.strip().lower() == 'fasttext':\n",
    "        download_path = \"fasttext-wiki-news-subwords-300\"\n",
    "    else:\n",
    "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove, fasttext\")\n",
    "        \n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Word2Vec: 300\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        print('FastText: 300')\n",
    "        raise e\n",
    "\n",
    "    return emb_model\n",
    "\n",
    "embedding_model = load_embedding_model(model_type=\"glove\",\n",
    "                                       embedding_dimension=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV terms: 3745 (3.98%)\n"
     ]
    }
   ],
   "source": [
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                    word_listing):\n",
    "    \"\"\"\n",
    "    Checks differences between pre-trained embedding model vocabulary\n",
    "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_listing: dataset specific vocabulary (list)\n",
    "\n",
    "    :return\n",
    "        - list of OOV terms\n",
    "    \"\"\"\n",
    "    # embedding_vocabulary = set(embedding_model.vocab.keys())\n",
    "    embedding_vocabulary = set(embedding_model.index_to_key)\n",
    "    oov = set(word_listing).difference(embedding_vocabulary)\n",
    "    return list(oov)\n",
    "\n",
    "oov_terms = check_OOV_terms(embedding_model, data_frame['token'])\n",
    "\n",
    "oov_percentage = float(len(oov_terms)) * 100 / len(data_frame['token'])\n",
    "print(f\"Total OOV terms: {len(oov_terms)} ({oov_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94084/94084 [00:00<00:00, 1573018.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 11968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def build_vocabulary(df):\n",
    "    \"\"\"\n",
    "    Given a dataset, builds the corresponding word vocabulary.\n",
    "\n",
    "    :param df: dataset from which we want to build the word vocabulary (pandas.DataFrame)\n",
    "    :return:\n",
    "      - word vocabulary: vocabulary index to word\n",
    "      - inverse word vocabulary: word to vocabulary index\n",
    "      - word listing: set of unique terms that build up the vocabulary\n",
    "    \"\"\"\n",
    "    idx_to_word = OrderedDict()\n",
    "    word_to_idx = OrderedDict()\n",
    "    \n",
    "    curr_idx = 0\n",
    "\n",
    "    for token in tqdm(df['token']):\n",
    "        if token not in word_to_idx:\n",
    "            word_to_idx[token] = curr_idx\n",
    "            idx_to_word[curr_idx] = token\n",
    "            curr_idx += 1\n",
    "\n",
    "    word_listing = list(idx_to_word.values())\n",
    "    return idx_to_word, word_to_idx, word_listing\n",
    " \n",
    "idx_to_word, word_to_idx, word_listing = build_vocabulary(data_frame)\n",
    "print(f\"Size of the vocabulary: {len(word_listing)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11968/11968 [00:00<00:00, 176750.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (11968, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                           embedding_dimension: int,\n",
    "                           word_to_idx,\n",
    "                           vocab_size: int,\n",
    "                           oov_terms):\n",
    "    \"\"\"\n",
    "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "    :param vocab_size: size of the vocabulary\n",
    "    :param oov_terms: list of OOV terms (list)\n",
    "\n",
    "    :return\n",
    "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
    "    for word, idx in tqdm(word_to_idx.items()):\n",
    "        try:\n",
    "            embedding_vector = embedding_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_dimension = 50\n",
    "embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, len(word_to_idx), oov_terms)\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the corresponding embeddings to the data frames that belong to the sequenced word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embeddings = lambda words: [embedding_matrix[word_to_idx[word], :] for word in words]\n",
    "\n",
    "train_frame['embedding'] = get_embeddings(train_frame['token'])\n",
    "test_frame['embedding'] = get_embeddings(test_frame['token'])\n",
    "val_frame['embedding'] = get_embeddings(val_frame['token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-29 00:53:09.411749: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-29 00:53:13.227558: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 256)              183296    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 185,866\n",
      "Trainable params: 185,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# create Keras model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# add embedding layer with embedding matrix\n",
    "model.add(keras.Input(shape=(32, embedding_dimension)))\n",
    "\n",
    "# add LSTM layer\n",
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=128)))\n",
    "\n",
    "# add dense layer\n",
    "model.add(keras.layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m max_len \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m train_frame[\u001b[39m'\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues:\n\u001b[0;32m---> 11\u001b[0m     \u001b[39mif\u001b[39;00m(t\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m] \u001b[39m>\u001b[39m max_len):\n\u001b[1;32m     12\u001b[0m         max_len \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy', 'precision', 'recall'])\n",
    "\n",
    "model.fit([t for t in train_frame['embedding'].values], [t for t in train_frame['pos_ohe'].values], epochs=10, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit ('nlp-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83996dc0b81395b17a2f017c39a7462422633737611de1f2a39e1edfb0be2356"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
