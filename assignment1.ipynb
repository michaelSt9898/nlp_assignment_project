{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "The following tasks have to be done:\n",
    "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
    "*   Embed the words using GloVe embeddings\n",
    "*   Create a baseline model, using a simple neural architecture\n",
    "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
    "*   Evaluate your two best model\n",
    "*   Analyze the errors of your model\n",
    "## Split the data into the training-, test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to iterate over all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_iterator():\n",
    "    data_dir = Path('dependency_treebank')\n",
    "    for data_file in filter(lambda f: os.path.isfile(data_dir/f) and f.endswith('.dp'), os.listdir(data_dir)):\n",
    "        yield data_dir/data_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the text files using the file iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = []\n",
    "test_set = []\n",
    "val_set = []\n",
    "\n",
    "train_split = 100\n",
    "val_split = 150\n",
    "\n",
    "file_counter = 0\n",
    "for file in file_iterator():\n",
    "    file_counter += 1\n",
    "    if file_counter <= train_split:\n",
    "        train_set.append(pd.read_csv(file, sep=\"\\t\", names=['token', 'pos'], usecols=[0, 1], engine='python'))\n",
    "    elif file_counter <= val_split:\n",
    "        val_set.append(pd.read_csv(file, sep=\"\\t\", names=['token', 'pos'], usecols=[0, 1], engine='python'))\n",
    "    else:\n",
    "        test_set.append(pd.read_csv(file, sep=\"\\t\", names=['token', 'pos'], usecols=[0, 1], engine='python'))\n",
    "\n",
    "assert len(train_set) == 100\n",
    "assert len(val_set) == 50\n",
    "assert len(test_set) == 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frame = pd.concat(train_set)\n",
    "test_frame = pd.concat(test_set)\n",
    "val_frame = pd.concat(val_set)\n",
    "\n",
    "assert sum([e.shape[0] for e in train_set]) == train_frame.shape[0]\n",
    "assert sum([e.shape[0] for e in test_set]) == test_frame.shape[0]\n",
    "assert sum([e.shape[0] for e in val_set]) == val_frame.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(model_type: str,\n",
    "                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    \"\"\"\n",
    "    Loads a pre-trained word embedding model via gensim library.\n",
    "\n",
    "    :param model_type: name of the word embedding model to load.\n",
    "    :param embedding_dimension: size of the embedding space to consider\n",
    "\n",
    "    :return\n",
    "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
    "    \"\"\"\n",
    "    download_path = \"\"\n",
    "    if model_type.strip().lower() == 'word2vec':\n",
    "        download_path = \"word2vec-google-news-300\"\n",
    "\n",
    "    elif model_type.strip().lower() == 'glove':\n",
    "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    elif model_type.strip().lower() == 'fasttext':\n",
    "        download_path = \"fasttext-wiki-news-subwords-300\"\n",
    "    else:\n",
    "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove, fasttext\")\n",
    "        \n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Word2Vec: 300\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        print('FastText: 300')\n",
    "        raise e\n",
    "\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = load_embedding_model(model_type=\"glove\",\n",
    "                                       embedding_dimension=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# check if vocabulary in embedding model is a superset of the vocabulary in the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mset\u001b[39m(train_frame[\u001b[39m'\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique())\u001b[39m.\u001b[39missubset(\u001b[39mset\u001b[39m(embedding_model\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39mkeys()))\n",
      "File \u001b[0;32m~/miniforge3/envs/play/lib/python3.9/site-packages/gensim/models/keyedvectors.py:735\u001b[0m, in \u001b[0;36mKeyedVectors.vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvocab\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 735\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    736\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe vocab attribute was removed from KeyedVector in Gensim 4.0.0.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse KeyedVector\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms .key_to_index dict, .index_to_key list, and methods \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m.get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    739\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    740\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "# check if vocabulary in embedding model is a superset of the vocabulary in the dataset\n",
    "assert set(train_frame['token'].unique()).issubset(set(embedding_model.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qr/scdb4s4n6dz5463spyd32yth0000gn/T/ipykernel_39179/3956582080.py:3: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  if word not in embedding_model.word_vec(word):\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Key 'In' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# check if vocabulary in embedding model is a superset of the vocabulary in the dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m train_frame[\u001b[39m'\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique():\n\u001b[0;32m----> 3\u001b[0m     \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m embedding_model\u001b[39m.\u001b[39;49mword_vec(word):\n\u001b[1;32m      4\u001b[0m         \u001b[39mprint\u001b[39m(word)\n",
      "File \u001b[0;32m~/miniforge3/envs/play/lib/python3.9/site-packages/gensim/utils.py:1522\u001b[0m, in \u001b[0;36mdeprecated.<locals>.decorator.<locals>.new_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m   1516\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_func1\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1517\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1518\u001b[0m         fmt\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, reason\u001b[39m=\u001b[39mreason),\n\u001b[1;32m   1519\u001b[0m         category\u001b[39m=\u001b[39m\u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m   1520\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m   1521\u001b[0m     )\n\u001b[0;32m-> 1522\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/play/lib/python3.9/site-packages/gensim/models/keyedvectors.py:460\u001b[0m, in \u001b[0;36mKeyedVectors.word_vec\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39m@deprecated\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUse get_vector instead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    458\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_vec\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    459\u001b[0m     \u001b[39m\"\"\"Compatibility alias for get_vector(); must exist so subclass calls reach subclass get_vector().\"\"\"\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_vector(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/play/lib/python3.9/site-packages/gensim/models/keyedvectors.py:447\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_vector\u001b[39m(\u001b[39mself\u001b[39m, key, norm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    424\u001b[0m     \u001b[39m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \n\u001b[1;32m    426\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m \n\u001b[1;32m    446\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 447\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_index(key)\n\u001b[1;32m    448\u001b[0m     \u001b[39mif\u001b[39;00m norm:\n\u001b[1;32m    449\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/miniforge3/envs/play/lib/python3.9/site-packages/gensim/models/keyedvectors.py:421\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[39mreturn\u001b[39;00m default\n\u001b[1;32m    420\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 421\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'In' not present\""
     ]
    }
   ],
   "source": [
    "# check if vocabulary in embedding model is a superset of the vocabulary in the dataset\n",
    "for word in train_frame['token'].unique():\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qr/scdb4s4n6dz5463spyd32yth0000gn/T/ipykernel_39179/840096791.py:1: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  type(embedding_model.word_vec('the')[0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embedding_model.word_vec('the')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                    word_listing):\n",
    "    \"\"\"\n",
    "    Checks differences between pre-trained embedding model vocabulary\n",
    "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_listing: dataset specific vocabulary (list)\n",
    "\n",
    "    :return\n",
    "        - list of OOV terms\n",
    "    \"\"\"\n",
    "    # embedding_vocabulary = set(embedding_model.vocab.keys())\n",
    "    embedding_vocabulary = set(embedding_model.index_to_key)\n",
    "    oov = set(word_listing).difference(embedding_vocabulary)\n",
    "    return list(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV terms: 2041 (4.52%)\n"
     ]
    }
   ],
   "source": [
    "oov_terms_train = check_OOV_terms(embedding_model, train_frame['token'])\n",
    "oov_percentage_train = float(len(oov_terms_train)) * 100 / len(train_frame['token'])\n",
    "print(f\"Total OOV terms: {len(oov_terms_train)} ({oov_percentage_train:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                           embedding_dimension: int,\n",
    "                           word_to_idx,\n",
    "                           vocab_size: int,\n",
    "                           oov_terms):\n",
    "    \"\"\"\n",
    "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "    :param vocab_size: size of the vocabulary\n",
    "    :param oov_terms: list of OOV terms (list)\n",
    "\n",
    "    :return\n",
    "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
    "    for word, idx in tqdm(word_to_idx.items()):\n",
    "        try:\n",
    "            embedding_vector = embedding_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45201/45201 [00:00<00:00, 3604790.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def build_vocabulary(df):\n",
    "    \"\"\"\n",
    "    Given a dataset, builds the corresponding word vocabulary.\n",
    "\n",
    "    :param df: dataset from which we want to build the word vocabulary (pandas.DataFrame)\n",
    "    :return:\n",
    "      - word vocabulary: vocabulary index to word\n",
    "      - inverse word vocabulary: word to vocabulary index\n",
    "      - word listing: set of unique terms that build up the vocabulary\n",
    "    \"\"\"\n",
    "    idx_to_word = OrderedDict()\n",
    "    word_to_idx = OrderedDict()\n",
    "    \n",
    "    curr_idx = 0\n",
    "\n",
    "    for token in tqdm(df['token']):\n",
    "        if token not in word_to_idx:\n",
    "            word_to_idx[token] = curr_idx\n",
    "            idx_to_word[curr_idx] = token\n",
    "            curr_idx += 1\n",
    "\n",
    "    word_listing = list(idx_to_word.values())\n",
    "    return idx_to_word, word_to_idx, word_listing\n",
    " \n",
    "idx_to_word, word_to_idx, word_listing = build_vocabulary(train_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7404/7404 [00:00<00:00, 253040.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (7404, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "embedding_dimension = 50\n",
    "embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, len(word_to_idx), oov_terms_train)\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7404"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 50)          370200    \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 256)              183296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 556,066\n",
      "Trainable params: 185,866\n",
      "Non-trainable params: 370,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "# create Keras model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# add embedding layer with embedding matrix\n",
    "model.add(keras.layers.Embedding(input_dim=len(word_to_idx),\n",
    "                                    output_dim=embedding_dimension,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    trainable=False))\n",
    "\n",
    "# add input layer for embedding matrix\n",
    "# model.add(keras.layers.Input(shape=(None, )))                                \n",
    "\n",
    "# add LSTM layer\n",
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=128), input_shape=(None, embedding_dimension)))\n",
    "\n",
    "# add dense layer\n",
    "model.add(keras.layers.Dense(10))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-25 13:18:28.681882: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 10) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [64], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m                 optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m                 metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_frame[\u001b[39m'\u001b[39;49m\u001b[39mtoken\u001b[39;49m\u001b[39m'\u001b[39;49m], train_frame[\u001b[39m'\u001b[39;49m\u001b[39mpos\u001b[39;49m\u001b[39m'\u001b[39;49m], epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/play/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/qr/scdb4s4n6dz5463spyd32yth0000gn/T/__autograph_generated_filez7dz4v1p.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/anani/miniforge3/envs/play/lib/python3.9/site-packages/keras/backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 10) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# train model\n",
    "model.fit(train_frame['token'], train_frame['pos'], epochs=10, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('play')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b727ce1e70db608c3383dbdfc7515fdf59394b92aeb6660adc1fd5fe991747d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
